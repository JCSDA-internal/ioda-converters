#!/usr/bin/env python

from collections import defaultdict, namedtuple
import sys
import os
import argparse
import yaml
import io

sys.path.append("@SCRIPT_LIB_PATH@")
import ioda_conv_ncio as iconv
import ioda_conv_util
import var_convert

# NOTE: As of December 11, 2018, the ODB API python package is built into the Singularity image and
#      put in /usr/local/lib/python2.7/dist-packages/odb, so the code below regarding the path
#      is unneeded in that environment (but it doesn't hurt anything).
#      As of March 28, 2019, the ODB API python module is in the python path when you load the
#      jedi/gnu-mpt module on Cheyenne.
#      If not using these environments, then note ODB API must be built on the system with the
#      ENABLE_PYTHON flag on or else the python module won't be there.
odbPythonPath = os.getenv('ODBPYTHONPATH', os.environ['HOME'] + '/projects/odb/install/lib/python2.7/site-packages/odb')
# print odbPythonPath

try:
    sys.path.index(odbPythonPath)
except ValueError:
    sys.path.append(odbPythonPath)

import odb

# Define some (global) frequently-used strings as variables to prevent the creation of new
# string objects every time they're used (often inside large loops).
date_s = "date"
time_s = "time"
andate_s = "andate"
antime_s = "antime"
vertco_type_s = "vertco_type"
vertco_reference_1_s = "vertco_reference_1"
analysis_date_time_s = "analysis_date_time"
date_time_s = "datetime"
float_s = "float"
string_s = "string"
varnoVertco_s = "USE_VERTCO"
IODA_MISSING_VAL = 1.0e9  # IODA converts any value larger than 1e8 to "Missing Value"


def CreateKeyTuple(keyDefinitionDict, row, selectColumns, ColumnVarDict, VertcoVarDict):
    returnKey = []
    for keyVariableName in keyDefinitionDict:
        keyVariableValue = None
        if keyVariableName in ColumnVarDict:
            keyVariableValue = row[selectColumns.index(ColumnVarDict[keyVariableName])]
        elif keyVariableName in VertcoVarDict:
            if row[selectColumns.index(vertco_type_s)] == VertcoVarDict[keyVariableName]:
                keyVariableValue = row[selectColumns.index(vertco_reference_1_s)]
                # If we're returning a pressure value, we divide by 100 to convert from Pa to hPa.
                # vertco_type=1 ==> pressure, vertco_type=11 ==> derived pressure (from aircraft altitude)
                if (VertcoVarDict[keyVariableName] == 1 or VertcoVarDict[keyVariableName] == 11) and keyVariableValue is not None:
                    keyVariableValue /= 100.0
        elif keyVariableName == analysis_date_time_s:
            keyVariableValue = ioda_conv_util.IntDateTimeToString(row[selectColumns.index(andate_s)], row[selectColumns.index(antime_s)])
        elif keyVariableName == date_time_s:
            keyVariableValue = ioda_conv_util.IntDateTimeToString(row[selectColumns.index(date_s)], row[selectColumns.index(time_s)])

        if keyVariableValue is None:
            if keyDefinitionDict[keyVariableName] == float_s:
                keyVariableValue = IODA_MISSING_VAL
            elif keyDefinitionDict[keyVariableName] == string_s:
                keyVariableValue = ""

        if keyDefinitionDict[keyVariableName] == string_s:
            keyVariableValue = keyVariableValue.rstrip()
        returnKey.append(keyVariableValue)
    returnKey = tuple(returnKey)
    return returnKey


###################################################################################
# MAIN
###################################################################################
ScriptName = os.path.basename(sys.argv[0])

# Parse command line
ap = argparse.ArgumentParser()
ap.add_argument("input_odb2", help="path to input ODB-2 file")
ap.add_argument("input_def", help="path to input yaml definition file")
ap.add_argument("output_netcdf", help="path to output netCDF4 file")
ap.add_argument("-c", "--clobber", action="store_true",
                help="allow overwrite of output netcdf file")
ap.add_argument("-q", "--qcfilter", action="store_true",
                help="only export rows with good qc flags")
ap.add_argument("-t", "--trace", action="store_true",
                help="Print trace statements")
ap.add_argument("-v", "--convertvars", action="store_true",
                help="Convert relative_humidity to specific_humidity and output both")
ap.add_argument("-b", "--usecorvalue", action="store_true",
                help="Copy 'corvalue' column (bias-corrected value) instead of 'obsvalue' column")

MyArgs = ap.parse_args()

# ObsType = MyArgs.obs_type
Odb2Fname = MyArgs.input_odb2
DefFname = MyArgs.input_def
NetcdfFname = MyArgs.output_netcdf
ClobberOfile = MyArgs.clobber
qcFilter = MyArgs.qcfilter
Trace = MyArgs.trace
ConvertVars = MyArgs.convertvars
UseCorvalue = MyArgs.usecorvalue

# Check files
BadArgs = False
if (not os.path.isfile(Odb2Fname)):
    print("ERROR: {0:s}: Specified input file does not exist: {1:s}".format(ScriptName, Odb2Fname))
    print("")
    BadArgs = True

if (not os.path.isfile(DefFname)):
    print("ERROR: {0:s}: Specified definition file does not exist: {1:s}".format(ScriptName, DefFname))
    print("")
    BadArgs = True

if (os.path.isfile(NetcdfFname)):
    if (ClobberOfile):
        print("WARNING: {0:s}: Overwriting nc file: {1:s}".format(ScriptName, NetcdfFname))
        print("")
    else:
        print("ERROR: {0:s}: Specified nc file already exists: {1:s}".format(ScriptName, NetcdfFname))
        print("ERROR: {0:s}:   Use -c option to overwrite.".format(ScriptName))
        print("")
        BadArgs = True

if (BadArgs):
    sys.exit(2)

if Trace:
    import time
    import datetime
    print "Start: " + datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S')

# Read in the contents of the definition file
with io.open(DefFname, 'r') as defstream:
    importDef = yaml.load(defstream)

columnVariableDict = importDef.get('column_variables')
if Trace:
    print "column_variables:"
    print str(columnVariableDict)
vertcoVariableDict = importDef.get('vertco_variables')
if Trace:
    print "vertco_variables:"
    print str(vertcoVariableDict)
varnoVariableDict = importDef.get('varno_variables')
if Trace:
    print "varno_variables:"
    print str(varnoVariableDict)
varnoVertcoVariableDict = importDef.get('varno_vertco_variables')
if Trace:
    print "varno_vertco_variables:"
    print str(varnoVertcoVariableDict)

# Define strings that act as keys in yaml definition file.
# The passed yaml file must use these strings, of course.
# yamlColumnVariables = 'column_variables'
# yamlVarnoVariables = 'varno_variables'
# yamlVertcoVariables = 'vertco_variables'
yamlRecordKey = 'record_key'
yamlLocationKey = 'location_key'

# print columnVariableDict

# Assemble list of columns to select
selectColumns = []
selectColumns.append("varno")
selectColumns.append(vertco_type_s)
selectColumns.append(vertco_reference_1_s)
selectColumns.append(andate_s)
selectColumns.append(antime_s)
selectColumns.append(date_s)
selectColumns.append(time_s)
selectColumns.append("obsvalue")
selectColumns.append("obs_error")
selectColumns.append("report_status.active")
selectColumns.append("report_status.passive")
selectColumns.append("report_status.rejected")
selectColumns.append("report_status.blacklisted")
selectColumns.append("datum_status.active")
selectColumns.append("datum_status.passive")
selectColumns.append("datum_status.rejected")
selectColumns.append("datum_status.blacklisted")

if UseCorvalue:
    selectColumns.append("corvalue")
    if Trace:
        print "Copying corvalue column (bias-corrected) value instead of obsvalue column."

# The columns above are always selected.
# Next we add the columns that are required by the variables
# in the "column_variables" section of the yaml definition.
for varName in columnVariableDict:
    selectColumns.append(columnVariableDict[varName])

# Assemble the columns to select into a string we'll use in sql
selectColumnsString = ""
for index, columnName in enumerate(selectColumns):
    if (index > 0):
        selectColumnsString += ", "
    selectColumnsString += columnName

# Assemble the vertco_type 'where' clause we'll use in sql
vertcoTypeSqlString = ""
for index, vertcoVariable in enumerate(vertcoVariableDict):
    if (index > 0):
        vertcoTypeSqlString += " or "
    vertcoTypeSqlString += "vertco_type="+str(vertcoVariableDict[vertcoVariable])

# Assemble the varno 'where' clause we'll use in sql
varnoSqlString = ""
for index, varnoValue in enumerate(varnoVariableDict):
    if (index > 0):
        varnoSqlString += " or "
    varnoSqlString += "varno="+str(varnoValue)

recordKeyList = []
for varName in importDef[yamlRecordKey]:
    recordKeyList.append((varName, importDef[yamlRecordKey][varName]))

if Trace:
    print "Record Key: ", str(recordKeyList)

locationKeyList = []
for varName in importDef[yamlLocationKey]:
    locationKeyList.append((varName, importDef[yamlLocationKey][varName]))

if Trace:
    print "Location Key: ", str(locationKeyList)

# Instantiate a netcdf writer object, and get the obs data names from
# the writer object.
nc_writer = iconv.NcWriter(NetcdfFname, recordKeyList, locationKeyList)

ncOvalName = nc_writer.OvalName()
ncOerrName = nc_writer.OerrName()
ncOqcName = nc_writer.OqcName()

varCategories = [ncOvalName, ncOerrName, ncOqcName]

# The top-level dictionary is keyed by the fields in recordKeyList, which uniquely identifiies a record.
# The second-level dictionary is keyed by the fields in locationKeyList, which uniquely identifies a location
# The third (bottom) level is keyed by a variable name and contains the value of the variable at the location.
obsDataDictTree = defaultdict(lambda: defaultdict(dict))

conn = odb.connect(Odb2Fname)
c = conn.cursor()

# Construct the entire sql statement
sql = "select " + selectColumnsString + " from \"" + Odb2Fname + "\"" + \
    " where (" + vertcoTypeSqlString + ") and (" + varnoSqlString + ")"
if qcFilter:
    sql += " and report_status.active=1 and report_status.passive=0 and report_status.rejected=0 and" \
           " report_status.blacklisted=0 and datum_status.active=1 and datum_status.passive=0 and datum_status.rejected=0" \
           " and datum_status.blacklisted=0"
sql += ';'

if Trace:
    print "ODB API SQL statement:"
    print sql

c.execute(sql)

row = c.fetchone()
refDateTimeString = None

# Defining these sql row indexes outside the main loop prevents new objects
# from being created for each one every iteration.
rsActiveIndex = selectColumns.index("report_status.active")
rsPassiveIndex = selectColumns.index("report_status.passive")
rsRejectedIndex = selectColumns.index("report_status.rejected")
rsBlacklistIndex = selectColumns.index("report_status.blacklisted")
dsActiveIndex = selectColumns.index("datum_status.active")
dsPassiveIndex = selectColumns.index("datum_status.passive")
dsRejectedIndex = selectColumns.index("datum_status.rejected")
dsBlacklistIndex = selectColumns.index("datum_status.blacklisted")
dateIndex = selectColumns.index(date_s)
timeIndex = selectColumns.index(time_s)
obs_errorIndex = selectColumns.index("obs_error")
varnoIndex = selectColumns.index("varno")
vertcoRef1Index = selectColumns.index(vertco_reference_1_s)

obsvalueIndex = selectColumns.index("corvalue") if UseCorvalue else selectColumns.index("obsvalue")

rowCount = 0

while row is not None:
    if Trace:
        rowCount += 1
        if rowCount % 50000 == 0:
            print "Processed {} rows".format(rowCount)

    if (refDateTimeString is None):
        refDateTimeString = ioda_conv_util.IntDateTimeToString(row[selectColumns.index(andate_s)], row[selectColumns.index(antime_s)])
    obsDateTimeString = ioda_conv_util.IntDateTimeToString(row[dateIndex], row[timeIndex])
    # Encode the 8 QC bitfields in the ODB2 file into a single value for IODA
    # Flip the "active" flags so that zero is always the "good" value for all the flags,
    # and a qcVal of zero indicates a good datum.
    notReportStatusActive = 0 if row[rsActiveIndex] else 1
    notDatumStatusActive = 0 if row[dsActiveIndex] else 1
    qcVal = (notReportStatusActive * 128 +
             row[rsPassiveIndex]   *  64 +
             row[rsRejectedIndex]  *  32 +
             row[rsBlacklistIndex] *  16 +
             notDatumStatusActive  *   8 +
             row[dsPassiveIndex]   *   4 +
             row[dsRejectedIndex]  *   2 +
             row[dsBlacklistIndex])

    # For conventional obs, varName is usually found using only varno.
    # For satellite obs, must also look at vertco_reference_1 column to find the channel
    varName = varnoVariableDict[row[varnoIndex]]
    if Trace and rowCount == 1:
        print "first row varName: {}".format(varName)
    if varName == varnoVertco_s:
        if row[varnoIndex] in varnoVertcoVariableDict and int(row[vertcoRef1Index]) in varnoVertcoVariableDict[row[varnoIndex]]:
            varName = varnoVertcoVariableDict[row[varnoIndex]][int(row[vertcoRef1Index])]
            if Trace and rowCount == 1:
                print "first row vertcoVarName: {}".format(varName)
        else:
            # Here we don't have a variable defined in the yaml for the channel number. (vertco_reference_1 value).
            # Just move onto the next row.
            row = c.fetchone()
            continue

    recordKey = CreateKeyTuple(importDef[yamlRecordKey], row, selectColumns, columnVariableDict, vertcoVariableDict)
    locationKey = CreateKeyTuple(importDef[yamlLocationKey], row, selectColumns, columnVariableDict, vertcoVariableDict)

    ovalKey = varName, ncOvalName
    oerrKey = varName, ncOerrName
    oqcKey = varName, ncOqcName

    oval = row[obsvalueIndex] if row[obsvalueIndex] is not None else IODA_MISSING_VAL
    oerr = row[obs_errorIndex] if row[obs_errorIndex] is not None else IODA_MISSING_VAL
    if qcVal is None:
        qcVal = IODA_MISSING_VAL

    # Assignment code below is done this way for two reasons:
    # 1. Want to make sure all locations get into IODA, even if they only have
    #    missing values. (Preserve all the data we can.)
    # 2. There can be multiple entries in the file for each locationKey, but
    #    we can only keep one. So we choose an entry that is not null/missing, if present.
    if (ovalKey not in obsDataDictTree[recordKey][locationKey] or
            obsDataDictTree[recordKey][locationKey][ovalKey] == IODA_MISSING_VAL):
        obsDataDictTree[recordKey][locationKey][ovalKey] = oval
    if (oerrKey not in obsDataDictTree[recordKey][locationKey] or
            obsDataDictTree[recordKey][locationKey][oerrKey] == IODA_MISSING_VAL):
        obsDataDictTree[recordKey][locationKey][oerrKey] = oerr
    if (oqcKey not in obsDataDictTree[recordKey][locationKey] or
            obsDataDictTree[recordKey][locationKey][oqcKey] == IODA_MISSING_VAL):
        obsDataDictTree[recordKey][locationKey][oqcKey] = qcVal

    row = c.fetchone()

if Trace:
    print "Finished reading file. Now populating missing values."

# After all the data from the file is in the dictionary tree, populate "gaps" with IODA missing value.
for recordKey in obsDataDictTree:
    for locationKey in obsDataDictTree[recordKey]:
        for varno in varnoVariableDict:
            varName = varnoVariableDict[varno]
            if varName == varnoVertco_s:
                for vertcoVarName in varnoVertcoVariableDict[varno].values():
                    for varCat in varCategories:
                        if (vertcoVarName, varCat) not in obsDataDictTree[recordKey][locationKey]:
                            obsDataDictTree[recordKey][locationKey][vertcoVarName, varCat] = IODA_MISSING_VAL
            else:
                for varCat in varCategories:
                    if (varName, varCat) not in obsDataDictTree[recordKey][locationKey]:
                        obsDataDictTree[recordKey][locationKey][varName, varCat] = IODA_MISSING_VAL

if Trace:
    print "Finished populating missing values."

# For now, we allow converting relative humidity (varno=29) to specific humidity here.
# This code should be removed eventually, as this is not the right place to convert variables.
if ConvertVars and (29 in varnoVariableDict):
    if Trace:
        print "Converting relative humidity to specific humidity."
    relative_humidity_s = "relative_humidity"
    specific_humidity_s = "specific_humidity"
    temperature_s = "temperature"

    # In order to retrieve the pressure value from the location key, we need to know
    # what position of the tuple it's in.
    pressureIndex = -1
    for index, varName in enumerate(importDef[yamlLocationKey]):
        if varName == "air_pressure":
            pressureIndex = index
            break

    for recordKey in obsDataDictTree:
        for locationKey in obsDataDictTree[recordKey]:
            if (relative_humidity_s, ncOvalName) in obsDataDictTree[recordKey][locationKey]:
                obsDict = obsDataDictTree[recordKey][locationKey]
                rh = obsDict[(relative_humidity_s, ncOvalName)]
                rh_err = obsDict[(relative_humidity_s, ncOerrName)]
                t = obsDict.get((temperature_s, ncOvalName))
                p = locationKey[pressureIndex]
                if (t is not None and rh is not None and rh_err is not None and p is not None and
                        t != IODA_MISSING_VAL and rh != IODA_MISSING_VAL and rh_err != IODA_MISSING_VAL and p != IODA_MISSING_VAL):
                    q, q_err = var_convert.ConvertRelativeToSpecificHumidity(rh, rh_err, t, p)

                    obsDict[(specific_humidity_s, ncOvalName)] = q
                    obsDict[(specific_humidity_s, ncOerrName)] = q_err
                    obsDict[(specific_humidity_s, ncOqcName)] = obsDict[(relative_humidity_s, ncOqcName)]
                else:
                    obsDict[(specific_humidity_s, ncOvalName)] = IODA_MISSING_VAL
                    obsDict[(specific_humidity_s, ncOerrName)] = IODA_MISSING_VAL
                    obsDict[(specific_humidity_s, ncOqcName)] = obsDict[(relative_humidity_s, ncOqcName)]
    if Trace:
        print "Finished humidity conversion."

if Trace:
    print "Num records: ", len(obsDataDictTree)
    print "Last record key: ", recordKey
    print "Num Locations, last record: ", len(obsDataDictTree[recordKey])

# Call the writer. Pass in the reference date time string for writing the
# version 1 netcdf file. The reference date time string won't be necessary when
# we switch to the version 2 netcdf file.
AttrData = {
    'odb_version': 2,
    'date_time_string': refDateTimeString
}

(ObsVars, RecMdata, LocMdata, VarMdata) = nc_writer.ExtractObsData(obsDataDictTree)
nc_writer.BuildNetcdf(ObsVars, RecMdata, LocMdata, VarMdata, AttrData)

if Trace:
    print "End: " + datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S')
