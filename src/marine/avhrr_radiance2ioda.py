#!/usr/bin/env python3

#
# (C) Copyright 2019-2021 UCAR
#
# This software is licensed under the terms of the Apache Licence Version 2.0
# which can be obtained at http://www.apache.org/licenses/LICENSE-2.0.
#

import argparse
import netCDF4 as nc
from datetime import datetime, timedelta
import dateutil.parser
import numpy as np
from multiprocessing import Pool
import os

import pyiodaconv.ioda_conv_engines as iconv
from pyiodaconv.orddicts import DefaultOrderedDict

output_var_names = ["brightnessTemperature"]

locationKeyList = [
    ("latitude", "float"),
    ("longitude", "float"),
    ("dateTime", "long")
]

GlobalAttrs = {}

VarAttrs = DefaultOrderedDict(lambda: DefaultOrderedDict(dict))

DimDict = {}

VarDims = {}

chan_number = {3, 4, 5}

time_units = ''     # Will fill in below.


def read_input(input_args):
    """
    Reads/converts a single input file, performing optional thinning also.

    Arguments:

        input_args: A tuple of input filename and global_config
            input_file: The name of file to read
            global_config: structure for global arguments related to read

    Returns:

        A tuple of (obs_data, GlobalAttrs) needed by the IODA writer.
    """
    input_file = input_args[0]
    global_config = input_args[1]

    print("Reading ", input_file)
    ncd = nc.Dataset(input_file, 'r')

    global time_units
    # get the base time (should only have 1 or 2 time slots)
    time_base = ncd.variables['time'][:]
    time_units = ncd.variables['time'].units[-19:] + 'Z'
    s = list(time_units)
    s[10] = "T"
    time_units = 'seconds since ' + "".join(s)
    basetime = dateutil.parser.parse(ncd.variables['time'].units[-19:])

    # get some of the global attributes that we are interested in

    #    for v in ('platform', 'sensor', 'processing_level'):
    #        GlobalAttrs[v] = ncd.getncattr(v)

    # get the QC flags, and calculate a mask from the non-missing values
    # (since L3 files are mostly empty, fields need a mask applied immediately
    # to avoid using way too much memory)
    data_in = {}
    data_in['bad_pixel_mask'] = ncd.variables['bad_pixel_mask'][:].ravel()
    mask = data_in['bad_pixel_mask'] >= 0
    data_in['bad_pixel_mask'] = data_in['bad_pixel_mask'][mask]

    # Determine the lat/lon grid.
    lons = ncd.variables['longitude'][:].ravel()
    lats = ncd.variables['latitude'][:].ravel()

    len_grid = len(lons)*len(lats)
    lons, lats = np.meshgrid(lons, lats, copy=False)
    lons = np.tile(lons.ravel(), len(time_base)).ravel()[mask]
    lats = np.tile(lats.ravel(), len(time_base)).ravel()[mask]

    # calculate the basetime offsets
    time = np.tile(np.atleast_2d(time_base).T, (1, len_grid)).ravel()[mask]

    # load in all the other data and apply the missing value mask
    input_vars = (
        'scan_line_number',  # "scan line number"
        'bad_pixel_mask',  # quality flaq
        'scan_line_time',  # "time for the scan line in fractional hours"
        'solar_zenith_angle',
        'sensor_zenith_angle',
        'sensor_azimuth_angle',
        'solar_azimuth_angle',
        'temp_11_0um_nom',
        'temp_12_0um_nom',
        'temp_3_75um_nom',
        'temp_11_0um_nom_stddev_3x3')

    for v in input_vars:
        if v not in data_in:
            data_in[v] = ncd.variables[v][:].ravel()[mask]
            if 'scale_factor' in ncd.variables[v].__dict__:
                scale_factor = ncd.variables[v].scale_factor
                add_offset = ncd.variables[v].add_offset
                data_in[v] = scale_factor * data_in[v] + add_offset

    ncd.close()

    # Create a mask for optional random thinning
    np.random.seed(
        int((global_config['date']-datetime(1970, 1, 1)).total_seconds()))
    mask = np.random.uniform(size=len(lons)) > global_config['thin']

    # also, sometimes certain input variables have their own mask due to
    # missing values
    for v in input_vars:
        if np.ma.is_masked(data_in[v]):
            mask = np.logical_and(mask, np.logical_not(data_in[v].mask))

    # apply the masks for thinning and missing values
    time = time[mask]
    lons = lons[mask]
    lats = lats[mask]
    for v in input_vars:
        data_in[v] = data_in[v][mask]

    # create a string version of the date for each observation
    dates = []
    for i in range(len(lons)):
        dates.append(round(3600*data_in['scan_line_time'][i]))
    dates = np.array(dates, dtype=np.int64)

    # output values
    nchans = len(chan_number)
    obs_dim = (len(lons))
    val_tb = np.zeros((obs_dim, nchans))
    val_tb[:, 0] = data_in['temp_3_75um_nom']
    val_tb[:, 1] = data_in['temp_11_0um_nom']
    val_tb[:, 2] = data_in['temp_12_0um_nom']
    # there isn't std for every channels we use the same obs error for all chans
    err = np.zeros((obs_dim, nchans))
    err[:, 0] = data_in['temp_11_0um_nom_stddev_3x3']
    err[:, 1] = data_in['temp_11_0um_nom_stddev_3x3']
    err[:, 2] = data_in['temp_11_0um_nom_stddev_3x3']
    qc = np.zeros((obs_dim, nchans))
    qc[:, 0] = data_in['bad_pixel_mask']
    qc[:, 1] = data_in['bad_pixel_mask']
    qc[:, 2] = data_in['bad_pixel_mask']

    # allocate space for output depending on which variables are to be saved

    obs_data = {}
    obs_data[('dateTime', 'MetaData')] = dates.astype(np.int64)
    obs_data[('latitude', 'MetaData')] = lats
    obs_data[('longitude', 'MetaData')] = lons
    obs_data[('sequenceNumber', 'MetaData')] = data_in['scan_line_number']
    obs_data[('height', 'MetaData')] = 840*np.ones((obs_dim)).astype('float32')
    obs_data[('sensorAzimuthAngle', 'MetaData')] = data_in['sensor_azimuth_angle']
    obs_data[('sensorZenithAngle', 'MetaData')] = data_in['sensor_zenith_angle']
    obs_data[('solarZenithAngle', 'MetaData')] = data_in['solar_zenith_angle']
    obs_data[('solarAzimuthAngle', 'MetaData')] = data_in['solar_azimuth_angle']
    obs_data[('scanPosition', 'MetaData')] = data_in['scan_line_number']
    obs_data[output_var_names[0], global_config['oval_name']] = val_tb.astype('float32')
    obs_data[output_var_names[0], global_config['oerr_name']] = err.astype('float32')
    obs_data[output_var_names[0], global_config['opqc_name']] = qc.astype('int32')

    return (obs_data, GlobalAttrs)


def main():

    # Get command line arguments
    parser = argparse.ArgumentParser(
        description=(
            'Reads the brightness temperature from any PATMOSX Data '
            ' and converts into IODA formatted output files.'
            ' Multiple files are concatenated ')
    )

    required = parser.add_argument_group(title='required arguments')
    required.add_argument(
        '-i', '--input',
        help="path of patmosx avhrr observation input file(s)",
        type=str, nargs='+', required=True)
    required.add_argument(
        '-o', '--output',
        help="path of IODA output file",
        type=str, required=True)
    required.add_argument(
        '-d', '--date',
        metavar="YYYYMMDDHH",
        help="base date for the center of the window",
        type=str, required=True)

    optional = parser.add_argument_group(title='optional arguments')
    optional.add_argument(
        '-t', '--thin',
        help="percentage of random thinning, from 0.0 to 1.0. Zero indicates"
             " no thinning is performed. (default: %(default)s)",
        type=float, default=0.0)
    optional.add_argument(
        '--threads',
        help='multiple threads can be used to load input files in parallel.'
             ' (default: %(default)s)',
        type=int, default=1)

    args = parser.parse_args()
    args.date = datetime.strptime(args.date, '%Y%m%d%H')

    # Setup the configuration that is passed to each worker process
    # Note: Pool.map creates separate processes, and can only take iterable
    # objects. Rather than using global variables, embed them into
    # the iterable object together with argument array passed in (args.input)
    global_config = {}
    global_config['date'] = args.date
    global_config['thin'] = args.thin
    global_config['oval_name'] = iconv.OvalName()
    global_config['oerr_name'] = iconv.OerrName()
    global_config['opqc_name'] = iconv.OqcName()

    pool_inputs = [(i, global_config) for i in args.input]

    # read / process files in parallel
    pool = Pool(args.threads)
    obs = pool.map(read_input, pool_inputs)

    # concatenate the data from the files
    obs_data, GlobalAttrs = obs[0]

    for i in range(1, len(obs)):
        for k in obs_data:
            axis = len(obs[i][0][k].shape)-1
            obs_data[k] = np.concatenate(
                (obs_data[k], obs[i][0][k]), axis=axis)

    # prepare global attributes we want to output in the file,
    # in addition to the ones already loaded in from the input file
    GlobalAttrs['datetimeReference'] = args.date.strftime("%Y-%m-%dT%H:%M:%SZ")
    GlobalAttrs['thinning'] = args.thin
    GlobalAttrs['converter'] = os.path.basename(__file__)

    # determine which variables we are going to output
    selected_names = []
    selected_names.append(output_var_names[0])

    # pass parameters to the IODA writer
    # (needed because we are bypassing ExtractObsData within BuildNetcdf)
    VarDims = {
        'brightnessTemperature': ['Location', 'Channel']
    }

    nchans = len(chan_number)
    nlocs = len(obs_data[('longitude', 'MetaData')])
    DimDict = {
        'Location': nlocs,
        'Channel': list(chan_number)
    }

    writer = iconv.IodaWriter(args.output, locationKeyList, DimDict)

    VarAttrs[('dateTime', 'MetaData')]['units'] = time_units
    VarAttrs[('brightnessTemperature', 'ObsValue')]['units'] = 'K'
    VarAttrs[('brightnessTemperature', 'ObsError')]['units'] = 'K'
    VarAttrs[('brightnessTemperature', 'ObsValue')]['_FillValue'] = 999
    VarAttrs[('brightnessTemperature', 'ObsError')]['_FillValue'] = 999
    VarAttrs[('brightnessTemperature', 'PreQC')]['_FillValue'] = 999

    writer.BuildIoda(obs_data, VarDims, VarAttrs, GlobalAttrs)


if __name__ == '__main__':
    main()
